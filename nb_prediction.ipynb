{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xgboost as xgb\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('data/perfumes.pkl')\n",
    "df = pd.read_pickle('encoded_perfumes.pkl')\n",
    "df.drop(columns=[\"collection\", \"top_notes\", \"middle_notes\", \"base_notes\", \"all_notes\"], inplace=True)\n",
    "df.drop(columns=[\"url\", \"Duft.number_of_ratings\", \"Haltbarkeit.number_of_ratings\", \"Sillage.number_of_ratings\", \"Flakon.number_of_ratings\", \"Preis-Leistungs-Verhältnis.number_of_ratings\"], inplace=True)\n",
    "\n",
    "drop_list = []\n",
    "for i in range(0,11):\n",
    "    drop_list.append(f'scent.{i}')\n",
    "    drop_list.append(f'sillage.{i}')\n",
    "    drop_list.append(f'durability.{i}')\n",
    "    drop_list.append(f'pricing.{i}')\n",
    "    drop_list.append(f'bottle.{i}')\n",
    "\n",
    "df.drop(columns=drop_list, inplace=True)\n",
    "\n",
    "\n",
    "encoded_dfs = [df.copy()]\n",
    "\n",
    "for col in ['brand', 'perfumer', 'flakon_designer']:\n",
    "    df[col] = df[col].apply(lambda x: x if pd.notna(x) else None)\n",
    "    dummies = pd.get_dummies(df[col], prefix=col, prefix_sep='_', dummy_na=False).astype(int)\n",
    "    encoded_dfs.append(dummies)\n",
    "\n",
    "# Concatenate all dataframes horizontally\n",
    "df_encoded = pd.concat(encoded_dfs, axis=1)\n",
    "df = df_encoded\n",
    "\n",
    "\n",
    "numeric_df = df.drop(columns=['name', 'brand', 'year', 'flakon_designer', 'perfumer'])\n",
    "feature_df = numeric_df.drop(columns=['Duft.rating', 'Haltbarkeit.rating', 'Sillage.rating', 'Flakon.rating', 'Preis-Leistungs-Verhältnis.rating'])\n",
    "\n",
    "feature_df.fillna(0, inplace=True)\n",
    "\n",
    "feature_df.info\n",
    "feature_df.isna().any().any()\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_df # Numeric features\n",
    "y = df['Duft.rating']  # Target column\n",
    "\n",
    "# Drop rows where Duft.rating is NaN\n",
    "mask = ~y.isna()  # Keep only rows where y is not NaN\n",
    "X = X[mask]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, out):\n",
    "    device = torch.device(\"mps\")  # Use MPS for Mac GPU acceleration\n",
    "\n",
    "    # Drop NaN values\n",
    "    mask = ~out.isna()\n",
    "    X = inp[mask].to_numpy()\n",
    "    y = out[mask].to_numpy()\n",
    "\n",
    "\n",
    "    # Convert to PyTorch tensors and move to GPU\n",
    "    X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Normalize data\n",
    "    X_mean, X_std = X.mean(), X.std()\n",
    "    X_norm = (X - X_mean) / X_std\n",
    "\n",
    "    y_mean, y_std = y.mean(), y.std()\n",
    "    y_norm = (y - y_mean) / y_std\n",
    "\n",
    "    # Split into train and test sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_norm.cpu().numpy(), \n",
    "                                                         y_norm.cpu().numpy(), \n",
    "                                                         test_size=0.2, \n",
    "                                                         random_state=42)\n",
    "\n",
    "    # Convert train-test sets back to PyTorch tensors and move to GPU\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Compute closed-form solution (θ = (X^T * X)^(-1) * X^T * y)\n",
    "    theta_hat = torch.linalg.pinv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = X_test @ theta_hat\n",
    "\n",
    "    # Compute Mean Squared Error\n",
    "    mse = torch.mean((y_test - y_pred) ** 2)\n",
    "    print(f\"Test Set MSE: {mse.item():.4f}\")\n",
    "\n",
    "    # Find top 10 most influential features\n",
    "    theta_abs = torch.abs(theta_hat)\n",
    "    top_indices = torch.argsort(theta_abs, descending=True)[:10]\n",
    "\n",
    "    print(\"Top 10 most influential features:\")\n",
    "    print(inp.columns[top_indices.cpu().numpy()])  # Convert indices to feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(feature_df, df['Duft.rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(feature_df, df['Haltbarkeit.rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(feature_df, df['Sillage.rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(feature_df, df['Preis-Leistungs-Verhältnis.rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = feature_df  # Numeric features\n",
    "# y = df['Duft.rating']  # Target column\n",
    "\n",
    "# # Drop rows where Duft.rating is NaN\n",
    "# mask = ~y.isna()  # Keep only rows where y is not NaN\n",
    "# X = X[mask]\n",
    "# y = y[mask]\n",
    "\n",
    "# X_np = X.to_numpy().astype(np.float32)  # Ensure float32 for PyTorch\n",
    "# y_np = y.to_numpy().astype(np.float32).reshape(-1, 1)  # Reshape for single output\n",
    "\n",
    "# # Split into train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train)\n",
    "# y_train_tensor = torch.tensor(y_train)\n",
    "# X_test_tensor = torch.tensor(X_test)\n",
    "# y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# batch_size = 16\n",
    "# train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Define Neural Network\n",
    "# class NeuralNet(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(NeuralNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 64)  # First layer with 64 neurons\n",
    "#         self.fc2 = nn.Linear(64, 32)  # Second layer with 32 neurons\n",
    "#         self.fc3 = nn.Linear(32, 1)  # Output layer\n",
    "#         self.relu = nn.LeakyReLU(0.01)  # Activation function\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)  # No activation here (Regression)\n",
    "#         return x\n",
    "\n",
    "# # Initialize model\n",
    "# input_dim = X_train.shape[1]\n",
    "# model = NeuralNet(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss and Optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Adam optimizer\n",
    "\n",
    "# # Training Loop\n",
    "# num_epochs = 150  # Adjust epochs if needed\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(X_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     if (epoch+1) % 10 == 0:  # Print every 10 epochs\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     y_test_pred = model(X_test_tensor)\n",
    "#     test_loss = criterion(y_test_pred, y_test_tensor).item()\n",
    "    \n",
    "# print(f\"Test MSE: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# y_test_pred_np = y_test_pred.numpy()\n",
    "# r2 = r2_score(y_test, y_test_pred_np)\n",
    "# print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare data (same as before)\n",
    "# X = feature_df  # Numeric features\n",
    "# y = df['Duft.rating']  # Target column\n",
    "\n",
    "# # Drop rows where Duft.rating is NaN\n",
    "# mask = ~y.isna()  # Keep only rows where y is not NaN\n",
    "# X = X[mask]\n",
    "# y = y[mask]\n",
    "\n",
    "# X_np = X.to_numpy().astype(np.float32)  # Ensure float32 for PyTorch\n",
    "# y_np = y.to_numpy().astype(np.float32).reshape(-1, 1)  # Reshape for single output\n",
    "\n",
    "# # Split into train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train)\n",
    "# y_train_tensor = torch.tensor(y_train)\n",
    "# X_test_tensor = torch.tensor(X_test)\n",
    "# y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "# # Dynamic Neural Network class that can adapt to hyperparameters\n",
    "# class DynamicNeuralNet(nn.Module):\n",
    "#     def __init__(self, input_size, n_layers, layer_sizes, activation):\n",
    "#         super(DynamicNeuralNet, self).__init__()\n",
    "        \n",
    "#         # Dictionary of available activation functions\n",
    "#         self.activations = {\n",
    "#             'relu': nn.ReLU(),\n",
    "#             'leaky_relu': nn.LeakyReLU(0.01),\n",
    "#             'tanh': nn.Tanh(),\n",
    "#             'sigmoid': nn.Sigmoid(),\n",
    "#             'elu': nn.ELU()\n",
    "#         }\n",
    "        \n",
    "#         # Set activation function\n",
    "#         self.activation = self.activations[activation]\n",
    "        \n",
    "#         # Create network architecture dynamically\n",
    "#         layers = []\n",
    "#         prev_size = input_size\n",
    "        \n",
    "#         # Create hidden layers\n",
    "#         for i in range(n_layers):\n",
    "#             layers.append(nn.Linear(prev_size, layer_sizes))\n",
    "#             layers.append(self.activation)\n",
    "#             prev_size = layer_sizes\n",
    "        \n",
    "#         # Output layer\n",
    "#         layers.append(nn.Linear(prev_size, 1))\n",
    "        \n",
    "#         # Sequential container for all layers\n",
    "#         self.network = nn.Sequential(*layers)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.network(x)\n",
    "\n",
    "# # Objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     # Hyperparameters to tune\n",
    "#     n_layers = trial.suggest_int('n_layers', 1, 5)  # Number of hidden layers\n",
    "#     layer_size = trial.suggest_int('layer_size', 16, 256)  # Size of each layer\n",
    "#     activation = trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'tanh', 'elu'])\n",
    "    \n",
    "#     # Optimizer selection\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD', 'RMSprop'])\n",
    "#     lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)  # Learning rate on log scale\n",
    "    \n",
    "#     # Other hyperparameters\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [8, 16, 32, 64])\n",
    "#     epochs = trial.suggest_int('epochs', 50, 300)\n",
    "    \n",
    "#     # Create model using the hyperparameters\n",
    "#     input_dim = X_train.shape[1]\n",
    "#     model = DynamicNeuralNet(input_dim, n_layers, layer_size, activation)\n",
    "    \n",
    "#     # Select optimizer\n",
    "#     if optimizer_name == 'Adam':\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     elif optimizer_name == 'AdamW':\n",
    "#         weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "#         optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     elif optimizer_name == 'SGD':\n",
    "#         momentum = trial.suggest_float('momentum', 0.0, 0.99)\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "#     else:  # RMSprop\n",
    "#         optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    \n",
    "#     # Loss function\n",
    "#     criterion = nn.MSELoss()\n",
    "    \n",
    "#     # Create DataLoaders\n",
    "#     train_loader = DataLoader(\n",
    "#         TensorDataset(X_train_tensor, y_train_tensor), \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=True\n",
    "#     )\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         # Optuna has a built-in pruning mechanism to stop unpromising trials\n",
    "#         if epoch % 10 == 0:\n",
    "#             # Evaluate on validation set for pruning\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 y_val_pred = model(X_test_tensor)\n",
    "#                 val_loss = criterion(y_val_pred, y_test_tensor).item()\n",
    "            \n",
    "#             trial.report(val_loss, epoch)\n",
    "            \n",
    "#             # Handle pruning based on the intermediate result\n",
    "#             if trial.should_prune():\n",
    "#                 raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "#     # Final evaluation\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         y_test_pred = model(X_test_tensor)\n",
    "#         test_loss = criterion(y_test_pred, y_test_tensor).item()\n",
    "#         r2 = r2_score(y_test.flatten(), y_test_pred.numpy().flatten())\n",
    "    \n",
    "#     # We want to maximize R², so return negative loss or return R² directly\n",
    "#     return r2  # Optuna minimizes by default, but since we return R², it will maximize it\n",
    "\n",
    "# # Create a study object and optimize the objective function\n",
    "# study = optuna.create_study(\n",
    "#     direction='maximize',  # We want to maximize R²\n",
    "#     sampler=TPESampler(),  # Use Tree-structured Parzen Estimator for Bayesian optimization\n",
    "#     pruner=optuna.pruners.MedianPruner()  # Pruning mechanism\n",
    "# )\n",
    "# study.optimize(objective, n_trials=50)  # Adjust number of trials based on your computation budget\n",
    "\n",
    "# # Print the best hyperparameters and score\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "# print(f\"  Value (R²): {trial.value:.4f}\")\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n",
    "\n",
    "# # Train the final model with the best hyperparameters\n",
    "# best_params = study.best_params\n",
    "\n",
    "# # Create final model\n",
    "# input_dim = X_train.shape[1]\n",
    "# final_model = DynamicNeuralNet(\n",
    "#     input_dim, \n",
    "#     best_params['n_layers'], \n",
    "#     best_params['layer_size'], \n",
    "#     best_params['activation']\n",
    "# )\n",
    "\n",
    "# # Setup best optimizer\n",
    "# if best_params['optimizer'] == 'Adam':\n",
    "#     final_optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "# elif best_params['optimizer'] == 'AdamW':\n",
    "#     final_optimizer = optim.AdamW(\n",
    "#         final_model.parameters(), \n",
    "#         lr=best_params['lr'], \n",
    "#         weight_decay=best_params['weight_decay']\n",
    "#     )\n",
    "# elif best_params['optimizer'] == 'SGD':\n",
    "#     final_optimizer = optim.SGD(\n",
    "#         final_model.parameters(), \n",
    "#         lr=best_params['lr'], \n",
    "#         momentum=best_params['momentum']\n",
    "#     )\n",
    "# else:  # RMSprop\n",
    "#     final_optimizer = optim.RMSprop(final_model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "# # Loss function\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Create DataLoader with best batch size\n",
    "# train_loader = DataLoader(\n",
    "#     TensorDataset(X_train_tensor, y_train_tensor), \n",
    "#     batch_size=best_params['batch_size'], \n",
    "#     shuffle=True\n",
    "# )\n",
    "# test_loader = DataLoader(\n",
    "#     TensorDataset(X_test_tensor, y_test_tensor), \n",
    "#     batch_size=best_params['batch_size'], \n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# # Train the final model\n",
    "# for epoch in range(best_params['epochs']):\n",
    "#     final_model.train()\n",
    "#     total_loss = 0\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         final_optimizer.zero_grad()\n",
    "#         y_pred = final_model(X_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         loss.backward()\n",
    "#         final_optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     if (epoch+1) % 10 == 0:\n",
    "#         print(f\"Epoch {epoch+1}/{best_params['epochs']}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# # Final evaluation\n",
    "# final_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     y_test_pred = final_model(X_test_tensor)\n",
    "#     test_loss = criterion(y_test_pred, y_test_tensor).item()\n",
    "    \n",
    "# print(f\"Test MSE: {test_loss:.4f}\")\n",
    "\n",
    "# # Calculate and print R² score\n",
    "# y_test_pred_np = y_test_pred.numpy()\n",
    "# r2 = r2_score(y_test, y_test_pred_np)\n",
    "# print(f\"Final R² Score: {r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Save the best model\n",
    "# torch.save(final_model.state_dict(), 'best_duft_rating_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "plt.subplot(1, 2, 2)\n",
    "optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature dataframe\n",
    "X = feature_df  # Numeric features\n",
    "y = df['Duft.rating']  # Target column\n",
    "\n",
    "# Drop rows where Duft.rating is NaN\n",
    "mask = ~y.isna()  # Keep only rows where y is not NaN\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_np = X.to_numpy().astype(np.float16)\n",
    "y_np = y.to_numpy().astype(np.float16)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to XGBoost DMatrix format (for efficiency)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Optuna objective function for XGBoost\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"tree_method\": \"hist\",  # Faster training\n",
    "        \"n_jobs\": -1,\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 10.0, log=True),  # L2 regularization\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),   # L1 regularization\n",
    "    }\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    model = xgb.train(params, dtrain, num_boost_round=params[\"n_estimators\"])\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(dtest)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return r2  # Optuna will maximize R²\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "best_params = study.best_trial.params\n",
    "print(f\"  Best R² Score: {study.best_trial.value:.4f}\")\n",
    "print(\"  Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = xgb.train(best_params, dtrain, num_boost_round=best_params[\"n_estimators\"])\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred_final = final_model.predict(dtest)\n",
    "final_r2 = r2_score(y_test, y_pred_final)\n",
    "\n",
    "print(f\"Final Test R² Score: {final_r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "final_model.save_model(\"best_xgboost_duft_rating.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
